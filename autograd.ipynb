{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ZgXFHM9zNmuVPJhjk6M-InWkeV1t_F1g","timestamp":1684888669461}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 03 Autograd\n","\n","In this part, learn about the Autograd and how to compute gradients."],"metadata":{"id":"WqxG16r1BNdM"}},{"cell_type":"markdown","source":["# What is Autograd?\n","\n","#Autogradとは\n","\n","Autograd is a PyTorch package that automatically calculates gradients, which are crucial for optimizing models.\n","\n","Grad is gradation, which is a vector of partial differentials with respect to each parameter.\n","\n","\n","Autogradは、モデルの最適化に欠かせない勾配を自動計算するPyTorchのパッケージです。\n","\n","gradは勾配のことで，各パラメータについての偏微分をベクトルとしてまとめたものです．\n"],"metadata":{"id":"GEIiXy8kG7cV"}},{"cell_type":"markdown","source":["# Autograd package\n","\n","Autograd provides automatic differentiation for all operations on Tensors.\n","\n","If we want the gradient, we have to set `requires_grad=True.` \n","\n","- This attribute set, all operations on the tensor are tracked in the computational graph.\n","\n","Operations with tensors create a computational graph and have a \"grad_fn\" attribute.\n","- In this program, \"y\" has a computational graph and \"grad_fn\" attribute.\n","\n","\n","[torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html): Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 \n","\n","\n","AutogradはTensorに対するすべての操作に対して自動で微分を計算します．\n","\n","勾配を求めたい場合は，`requires_grad=True.`を設定する必要があります．\n","\n","- この属性が設定されると，Tensorに対するすべての操作が計算グラフの中で追跡されるようになります．\n","\n","Tensorを使った操作は計算グラフを作り、\"grad_fn\" 属性を持ちます．\n","- このプログラムでは、\"y \"は計算グラフと \"grad_fn \"属性を持っています．\n","\n","\n","[torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html)： 平均0、分散1の正規分布からの乱数で満たされたTensorを返します．\n","\n","\n","\n"],"metadata":{"id":"_dR-r_tkMWkr"}},{"cell_type":"code","source":["import torch\n","x = torch.randn(3, requires_grad=True)\n","y = x + 2\n","\n","\n","print(x)\n","print(y)\n","print(y.grad_fn)\n","\n","\n","z = y * y * 3\n","print(z)\n","z = z.mean()\n","print(z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZUAkqqWWLyPX","executionInfo":{"status":"ok","timestamp":1684872279000,"user_tz":-540,"elapsed":459,"user":{"displayName":"たゆ","userId":"01723742488000179018"}},"outputId":"eb0ac46c-ada0-4460-f29f-108f9a76aa83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-0.3206,  0.7642, -0.7820], requires_grad=True)\n","tensor([1.6794, 2.7642, 1.2180], grad_fn=<AddBackward0>)\n","<AddBackward0 object at 0x7fcc013177c0>\n","tensor([ 8.4616, 22.9223,  4.4503], grad_fn=<MulBackward0>)\n","tensor(11.9447, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"markdown","source":["# Computational graph\n","\n","Operations with tensors create a computational graph.\n","\n","First, do a forward pass, calculate the output Y.\n","- If `requires_grad = True`: automatically create and store a function.  and this function is then used in the back propagation.\n","\n","- In this program, `y` has an attribute grad.\n","\n","This function(grad_fn) is used in the back propagation and to get the gradients \n","\n","- `y` has an attribute grad_fm so this will point to a gradient function\n","\n","\n","It's called at `add backward` and with this function we can calculate the gradients.\n","\n","\n","Tensorを使った演算は，計算グラフを作成します．\n","\n","まず，forward passで.，出力Yを計算します．\n","- requires_grad = True` の場合：自動的に関数を作成し，保存します．\n","\n","- このプログラムの`y`はgrad属性を持っています．\n","\n","この関数(grad_fn)はback propagationで使用され，gradientsを得るために使用されます．\n","\n","- y`はgrad_fm属性を持っており，これは勾配関数を指します．\n","\n","grad_fmが，`add backward`で呼び出され, この関数を使用し，gradientを計算できます．\n","\n","\n","![as](https://drive.google.com/uc?id=1H-cON2ukow9bAvcSa_E7sQVTRclX9H8y)\n"],"metadata":{"id":"H9XcelFXYx1H"}},{"cell_type":"markdown","source":["# compute the gradients with backpropagation\n","\n","When we finish our computation we can simply call .backward() and have all the gradients computed automatically. The gradient for this tensor will be accumulated into .grad attribute. It is the partial derivate of the function w.r.t. the tensor.\n","\n","\n","計算が終わったら，.backward()を呼び出すだけで，すべての勾配を自動的に計算させることができます．このTensorの勾配は.grad属性に累積されます．これはTensorに対する関数の偏導関数です．\n"],"metadata":{"id":"Y59ljwRCsOWt"}},{"cell_type":"code","source":["z.backward()\n","print(x.grad) # dz/dx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2kJoziLdHixd","executionInfo":{"status":"ok","timestamp":1684872284788,"user_tz":-540,"elapsed":620,"user":{"displayName":"たゆ","userId":"01723742488000179018"}},"outputId":"2d26968e-176a-4946-9221-5abf33673593"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3.3589, 5.5284, 2.4359])\n"]}]},{"cell_type":"markdown","source":["In general torch.autograd is computing a vector-Jacobian product. Compute partial derivatives while applying the chain rule.\n","\n","一般的にtorch.autogradはvector-Jacobian productというものを計算しています．連鎖律を適用しながら部分導関数を計算します．"],"metadata":{"id":"GsR5FyM3HrHX"}},{"cell_type":"code","source":["# Model with non-scalar output:\n","# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \n","\n","\n","x = torch.randn(3, requires_grad=True)\n","\n","y = x * 2\n","for _ in range(10):\n","    y = y * 2\n","\n","print(y)\n","print(y.shape)\n","\n","v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n","y.backward(v)\n","print(x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOsew_4AHsI1","executionInfo":{"status":"ok","timestamp":1684876135014,"user_tz":-540,"elapsed":585,"user":{"displayName":"たゆ","userId":"01723742488000179018"}},"outputId":"f8475faa-120f-4638-c03e-8d039565681f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-2066.9714, -2159.9631, -3006.5088], grad_fn=<MulBackward0>)\n","torch.Size([3])\n","tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"]}]},{"cell_type":"markdown","source":["# Stop a tensor from tracking history\n","\n","For example during training loop when we want to update our weights then this update operation should not be part of the gradient computation. We have 3 options to stop gradient calculations.\n","\n","例えば，学習ループ中に重みを更新する場合，この更新操作は勾配計算の一部であってはならない．勾配計算を停止するには，3つのオプションがあります．"],"metadata":{"id":"IiwC-jdKIJHJ"}},{"cell_type":"markdown","source":["# .requires_grad_(): changes an existing flag in-place\n","\n","# .requires_grad_(): 存在するフラグをその場で変更\n","\n","requires_grad_() controls whether the tensor is accessed directly and its gradient is tracked.\n","This modifies the tensor itself, so make sure the tensor is not shared with other variables. If you call a.requires_grad_(False), the gradient computation for a will stop.\n","\n","requires_grad_()は，Tensorに直接アクセスし，その勾配を追跡するかどうかを制御します．\n","これによりTensor自体が変更されるため，Tensorが他の変数と共有されていないか確認が必要です． もし，a.requires_grad_(False) を呼び出すと，a の勾配計算が停止します．"],"metadata":{"id":"v5ULET3IKqzb"}},{"cell_type":"code","source":["a = torch.randn(2, 2)\n","print(a.requires_grad)\n","b = ((a * 3) / (a - 1))\n","print(b.grad_fn)\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","b = (a * a).sum()\n","print(b.grad_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oARlvsFpIM2E","executionInfo":{"status":"ok","timestamp":1684873274692,"user_tz":-540,"elapsed":522,"user":{"displayName":"たゆ","userId":"01723742488000179018"}},"outputId":"3838e9fa-322f-41e2-ef1b-1cad9d4d8416"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","None\n","True\n","<SumBackward0 object at 0x7fcc01317d90>\n"]}]},{"cell_type":"markdown","source":["# .detach(): get a new Tensor with the same content but no gradient computation:\n","# .detach(): 同じ内容を持つが勾配計算のない新しい Tensor を取得\n","\n",".detach() creates a new tensor with the same data as the original tensor but without gradient chasing. if b = a.detach(), b will have the value of a, but the gradient computation will not be tracked.\n","\n",".detach()は，元のTensorと同じデータを持つが勾配追跡を行わない新しいTensorを作成します．b = a.detach()とすると，bはaの値を持ちますが，勾配の追跡はされません．"],"metadata":{"id":"LoyshIbxK01h"}},{"cell_type":"code","source":["a = torch.randn(2, 2, requires_grad=True)\n","print(a.requires_grad)\n","b = a.detach()\n","print(b.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gwfubXb6LAti","executionInfo":{"status":"ok","timestamp":1684872331482,"user_tz":-540,"elapsed":967,"user":{"displayName":"たゆ","userId":"01723742488000179018"}},"outputId":"caff6944-d10e-430a-95af-820d87e4be60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"markdown","source":["# wrap in with torch.no_grad():\n","# torch.no_grad()で包む\n","\n","Temporarily suspends gradient tracking for all internal calculations.\n","\n","内部で行われるすべての計算の勾配追跡を一時的に停止します．"],"metadata":{"id":"ehTOfhjKLGKk"}},{"cell_type":"code","source":["a = torch.randn(2, 2, requires_grad=True)\n","print(a.requires_grad)\n","with torch.no_grad():\n","    print((x ** 2).requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pb2df1XrLKqf","executionInfo":{"status":"ok","timestamp":1684872333763,"user_tz":-540,"elapsed":14,"user":{"displayName":"たゆ","userId":"01723742488000179018"}},"outputId":"a5198cc9-7eca-40ce-f9e3-2b4e9bc1c2cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"markdown","source":["# Empty gradients\n","backward() is accumulating the gradient of this tensor into the .grad attribute.\n","So we need to use .zero_() to empty the gradients before a new optimization step.\n","Otherwise, gradient information from different iterations may be mixed and training may not be correct.\n","\n","\n","backward() は、このTensorの勾配を .grad 属性に累積しています．\n","そのため，.zero_()を使って，新しい最適化ステップの前に勾配を空にする必要があります．\n","空にしないと，異なる反復からの勾配情報が混在し，学習が正しく行われなくなる可能性があります．"],"metadata":{"id":"wf4MPl1eLLqv"}},{"cell_type":"code","source":["weights = torch.ones(4, requires_grad=True)\n","\n","for epoch in range(3):\n","    # just a dummy example\n","    model_output = (weights*3).sum()\n","    model_output.backward()\n","\n","    print(weights.grad)\n","\n","    # optimize model, i.e. adjust weights...\n","    with torch.no_grad():\n","        weights -= 0.1 * weights.grad\n","\n","    # this is important! It affects the final weights & output\n","    weights.grad.zero_()\n","\n","print(weights)\n","print(model_output)"],"metadata":{"id":"za1n6tGjLSQQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684873279000,"user_tz":-540,"elapsed":735,"user":{"displayName":"たゆ","userId":"01723742488000179018"}},"outputId":"328ebf7c-96e8-4ee2-c0a9-8d448a0d8610"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n","tensor(4.8000, grad_fn=<SumBackward0>)\n"]}]}]}
